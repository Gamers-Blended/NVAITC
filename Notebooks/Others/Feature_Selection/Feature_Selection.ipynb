{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df872842",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "\n",
    "# 1. Univariate Selection\n",
    "Statistical tests can be used to select features that have the strongest relationship with the output variable.\n",
    "\n",
    "The `SelectKBest` class can be used with a suite of different statistical tests to select a specific number of features.\n",
    "\n",
    "For example, the ANOVA F-value method is appropriate for numerical inputs and categorical data. <br>\n",
    "Can be used via the `f_classif()` function. <br>\n",
    "Select the 4 best features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84707b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Univariate Statistical Tests\n",
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1eb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] # manually name all columns\n",
    "dataframe = read_csv(filename, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dee805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataframe.values\n",
    "X = array[:,0:8] # all rows, first 8 columns\n",
    "Y = array[:,8] # all rows, 9th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114baa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
      "[[  6.  148.   33.6  50. ]\n",
      " [  1.   85.   26.6  31. ]\n",
      " [  8.  183.   23.3  32. ]\n",
      " [  1.   89.   28.1  21. ]\n",
      " [  0.  137.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e68903",
   "metadata": {},
   "source": [
    "Scores for each attribute and the 4 attributes chosen (those with the highest scores). <br>\n",
    "Specifically features with indexes 0 (preq), 1 (plas), 5 (mass), and 7 (age)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecc060",
   "metadata": {},
   "source": [
    "# 2. Recursive Feature Elimination\n",
    "Recursive Feature Elimination (RFE) works by recursively removing attributes and building a model on remaining attributes.\n",
    "\n",
    "It uses model accuracy to identify which attributes (and attributes combination) contribute the most to predicting target attribute.\n",
    "\n",
    "RFE with logistic regression algorithm to select top 3 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82197fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with RFE\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a693226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = read_csv(url, names=names)\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba5eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "rfe = RFE(model, n_features_to_select=3, step=1)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebff2c",
   "metadata": {},
   "source": [
    "Top features are marked True in the support_ array and marked with a choice “1” in the ranking_ array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d143c7",
   "metadata": {},
   "source": [
    "# 3. Principal Component Analysis\n",
    "Principal Component Analysis (PCA) uses linear algebra to transform dataset into a compressed form (data reduction technique).\n",
    "\n",
    "A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.\n",
    "\n",
    "Select 3 principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c309715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with PCA\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6fe1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = read_csv(url, names=names)\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7d93717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a620e",
   "metadata": {},
   "source": [
    "# 4. Feature Importance\n",
    "Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b90fce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80f405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# dataframe = read_csv(url, names=names)\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9990b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.114 0.244 0.084 0.081 0.07  0.133 0.125 0.149]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb001632",
   "metadata": {},
   "source": [
    "The larger importance score the more important the attribute. <br>\n",
    "The scores suggest at the importance of plas, age and mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b9f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
